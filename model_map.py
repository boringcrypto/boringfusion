from core.data import ModelMap, ModelDataInfo

map = ModelMap({
    "5770c902": ModelDataInfo(
        "CLIPL14-fp16",
        "CLIP Encoder",
        "OpenAI CLIP ViT-L/14 fp16",
        "data/clip-text-encoder/5770c902-5770c902.bin",
        "5770c902",
        "5770c902",
        "torch.float16, torch.int64",
        "123060557",
        "https://github.com/openai/CLIP/blob/main/model-card.md",
        "https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin",
        "The base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets.",
        ['sd-wikiart-v2', 'bf_fb_v3_t4_b16_noadd-ema-pruned-fp16.ckpt', 'bondage3_24000.ckpt', 'easter_e5.ckpt', 'furry_epoch4.ckpt', 'gape22_yiffy15.ckpt', 'HD-16.ckpt', 'last-pruned.ckpt', 'LD-70k-1e-pruned.ckpt', 'Lewd-diffusion-pruned.ckpt', 'pyros-bj-v1-0.ckpt', 'r34_e4.ckpt', 'trinart2_step115000.ckpt', 'trinart2_step60000.ckpt', 'trinart2_step95000.ckpt', 'trinart_characters_it4_v1.ckpt', 'wd-v1-3-float16.ckpt', 'yiffy-e18.ckpt', 'Zack3D_Kinky-v1.ckpt', 'cafe-instagram-unofficial-test-epoch-9-140k-images-fp32.ckpt', 'arcane-diffusion-v2.ckpt', 'eldenring-v1-pruned.ckpt'],
    ),
    "8b408781": ModelDataInfo(
        "CLIPL14-fp32",
        "CLIP Encoder",
        "OpenAI CLIP ViT-L/14 fp32",
        "data/clip-text-encoder/5770c902-8b408781.bin",
        "5770c902",
        "8b408781",
        "torch.float32, torch.int64",
        "123060557",
        "https://github.com/openai/CLIP/blob/main/model-card.md",
        "https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin",
        "The base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. The model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets.",
        ['stable-diffusion-v1-4', 'bf_fb_v3_t4_b16_noadd-ema-pruned-fp32.ckpt', 'ema-only-epoch=000142.ckpt', 'gape60.ckpt', 'gg1342_testrun1_pruned.ckpt', 'LOKEAN_MISSIONARY_POV.ckpt', 'LOKEAN_PUPPYSTYLE_POV.ckpt', 'Mixed.ckpt', 'nai-full-latest.ckpt', 'nai-full-pruned.ckpt', 'sd-v1-1-full-ema.ckpt', 'sd-v1-1.ckpt', 'sd-v1-2-full-ema.ckpt', 'sd-v1-2.ckpt', 'sd-v1-3-full-ema.ckpt', 'sd-v1-3.ckpt', 'sd-v1-4-full-ema.ckpt', 'sd-v1-4.ckpt', 'sd-v1-5-inpainting.ckpt', 'v1-5-pruned-emaonly.ckpt', 'v1-5-pruned.ckpt', 'wd-v1-3-float32.ckpt', 'wd-v1-3-full-opt.ckpt', 'wd-v1-3-full.ckpt', 'SF_EB_1.0_ema_vae.ckpt'],
    ),
    "dacf2615": ModelDataInfo(
        "VAEEnc1_4_fp16",
        "VAE Encoder",
        "bf_fb_v3_t4_b16_noadd-ema-pruned-fp16.ckpt",
        "data/vae-encoder/dacf2615-dacf2615.bin",
        "dacf2615",
        "dacf2615",
        "torch.float16",
        "34163664",
        "",
        "",
        "",
        ['bf_fb_v3_t4_b16_noadd-ema-pruned-fp16.ckpt', 'bondage3_24000.ckpt', 'bukkake_20_training_images_2020_max_training_steps_woman_class_word.ckpt', 'Cyberpunk-Anime-Diffusion.ckpt', 'DCAUV1.ckpt', 'easter_e5.ckpt', 'furry_epoch4.ckpt', 'gape22_yiffy15.ckpt', 'Hiten girl_anime_8k_wallpaper_4k.ckpt', 'last-pruned.ckpt', 'LD-70k-1e-pruned.ckpt', 'Lewd-diffusion-pruned.ckpt', 'pyros-bj-v1-0.ckpt', 'r34_e4.ckpt', 'trinart2_step115000.ckpt', 'trinart2_step60000.ckpt', 'trinart2_step95000.ckpt', 'trinart_characters_it4_v1.ckpt', 'wd-v1-3-float16.ckpt', 'yiffy-e18.ckpt', 'Zack3D_Kinky-v1.ckpt', 'sd-wikiart-v2', 'cafe-instagram-unofficial-test-epoch-9-140k-images-fp32.ckpt', 'arcane-diffusion-5k.ckpt', 'arcane-diffusion-v2.ckpt', 'arcane-diffusion-v3.ckpt', 'classicAnim-v1.ckpt', 'eldenring-v1-pruned.ckpt', 'moDi-v1-pruned.ckpt', 'eldenring-v2-pruned.ckpt', 'eldenRing-v3-pruned.ckpt'],
    ),
    "56ea7f90": ModelDataInfo(
        "VAEEnc1_4_fp32",
        "VAE Encoder",
        "bf_fb_v3_t4_b16_noadd-ema-pruned-fp32.ckpt",
        "data/vae-encoder/dacf2615-56ea7f90.bin",
        "dacf2615",
        "56ea7f90",
        "torch.float32",
        "34163664",
        "",
        "",
        "",
        ['bf_fb_v3_t4_b16_noadd-ema-pruned-fp32.ckpt', 'ema-only-epoch=000142.ckpt', 'gape60.ckpt', 'gg1342_testrun1_pruned.ckpt', 'JSD-v1-4.ckpt', 'LOKEAN_MISSIONARY_POV.ckpt', 'LOKEAN_PUPPYSTYLE_POV.ckpt', 'Mixed.ckpt', 'nai-full-latest.ckpt', 'nai-full-pruned.ckpt', 'pachu_artwork_style_v1_iter8000.ckpt', 'robo-diffusion-v1.ckpt', 'sd-v1-1-full-ema.ckpt', 'sd-v1-1.ckpt', 'sd-v1-2-full-ema.ckpt', 'sd-v1-2.ckpt', 'sd-v1-3-full-ema.ckpt', 'sd-v1-3.ckpt', 'sd-v1-4-full-ema.ckpt', 'sd-v1-4.ckpt', 'sd-v1-5-inpainting.ckpt', 'v1-5-pruned-emaonly.ckpt', 'v1-5-pruned.ckpt', 'wd-v1-3-float32.ckpt', 'wd-v1-3-full-opt.ckpt', 'wd-v1-3-full.ckpt', 'stable-diffusion-v1-4', 'Taiyi-Stable-Diffusion-1B-Chinese-v0.1', 'SF_EB_1.0_ema_vae.ckpt', 'vae-ft-ema-560000-ema-pruned.ckpt', 'vae-ft-mse-840000-ema-pruned.ckpt', 'kl-f8_model.ckpt'],
    ),
    "97dd98b1": ModelDataInfo(
        "VAEDec1_4_fp16",
        "VAE Decoder",
        "bf_fb_v3_t4_b16_noadd-ema-pruned-fp16.ckpt",
        "data/vae-decoder/97dd98b1-97dd98b1.bin",
        "97dd98b1",
        "97dd98b1",
        "torch.float16",
        "49490199",
        "",
        "",
        "",
        ['bf_fb_v3_t4_b16_noadd-ema-pruned-fp16.ckpt', 'bondage3_24000.ckpt', 'bukkake_20_training_images_2020_max_training_steps_woman_class_word.ckpt', 'Cyberpunk-Anime-Diffusion.ckpt', 'DCAUV1.ckpt', 'easter_e5.ckpt', 'furry_epoch4.ckpt', 'gape22_yiffy15.ckpt', 'Hiten girl_anime_8k_wallpaper_4k.ckpt', 'last-pruned.ckpt', 'LD-70k-1e-pruned.ckpt', 'Lewd-diffusion-pruned.ckpt', 'pyros-bj-v1-0.ckpt', 'r34_e4.ckpt', 'trinart2_step115000.ckpt', 'trinart2_step60000.ckpt', 'trinart2_step95000.ckpt', 'trinart_characters_it4_v1.ckpt', 'wd-v1-3-float16.ckpt', 'yiffy-e18.ckpt', 'Zack3D_Kinky-v1.ckpt', 'sd-wikiart-v2', 'cafe-instagram-unofficial-test-epoch-9-140k-images-fp32.ckpt', 'arcane-diffusion-5k.ckpt', 'arcane-diffusion-v2.ckpt', 'arcane-diffusion-v3.ckpt', 'eldenring-v1-pruned.ckpt'],
    ),
    "8b7877f3": ModelDataInfo(
        "VAEDec1_4_fp32",
        "VAE Decoder",
        "bf_fb_v3_t4_b16_noadd-ema-pruned-fp32.ckpt",
        "data/vae-decoder/97dd98b1-8b7877f3.bin",
        "97dd98b1",
        "8b7877f3",
        "torch.float32",
        "49490199",
        "",
        "",
        "",
        ['bf_fb_v3_t4_b16_noadd-ema-pruned-fp32.ckpt', 'ema-only-epoch=000142.ckpt', 'gape60.ckpt', 'gg1342_testrun1_pruned.ckpt', 'JSD-v1-4.ckpt', 'LOKEAN_MISSIONARY_POV.ckpt', 'LOKEAN_PUPPYSTYLE_POV.ckpt', 'Mixed.ckpt', 'nai-full-latest.ckpt', 'nai-full-pruned.ckpt', 'robo-diffusion-v1.ckpt', 'sd-v1-1-full-ema.ckpt', 'sd-v1-1.ckpt', 'sd-v1-2-full-ema.ckpt', 'sd-v1-2.ckpt', 'sd-v1-3-full-ema.ckpt', 'sd-v1-3.ckpt', 'sd-v1-4-full-ema.ckpt', 'sd-v1-4.ckpt', 'sd-v1-5-inpainting.ckpt', 'v1-5-pruned-emaonly.ckpt', 'v1-5-pruned.ckpt', 'wd-v1-3-float32.ckpt', 'wd-v1-3-full-opt.ckpt', 'wd-v1-3-full.ckpt', 'stable-diffusion-v1-4', 'Taiyi-Stable-Diffusion-1B-Chinese-v0.1', 'kl-f8_model.ckpt'],
    ),
    "062d85c8": ModelDataInfo(
        "SD1.1-fp32",
        "UNet EMA",
        "Stable Diffusion v1.1 EMA fp32",
        "data/unet/7d1d7e93-062d85c8.bin",
        "7d1d7e93",
        "062d85c8",
        "torch.float32",
        "859520964",
        "https://huggingface.co/CompVis/stable-diffusion-v-1-1-original",
        "https://huggingface.co/CompVis/stable-diffusion-v-1-1-original/resolve/main/sd-v1-1.ckpt",
        "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v-1-1 was trained on 237,000 steps at resolution 256x256 on laion2B-en, followed by 194,000 steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024).",
        ['sd-v1-1-full-ema.ckpt', 'sd-v1-1.ckpt'],
    ),
    "161f6a95": ModelDataInfo(
        "SD1.2-fp32",
        "UNet EMA",
        "Stable Diffusion v1.2 EMA fp32",
        "data/unet/19de9b69-161f6a95.bin",
        "19de9b69",
        "161f6a95",
        "torch.float32",
        "859520964",
        "https://huggingface.co/CompVis/stable-diffusion-v-1-2-original",
        "https://huggingface.co/CompVis/stable-diffusion-v-1-2-original/resolve/main/sd-v1-2.ckpt",
        "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v-1-2 checkpoint was initialized with the weights of the Stable-Diffusion-v-1-1 checkpoint and subsequently fine-tuned on 515,000 steps at resolution 512x512 on 'laion-improved-aesthetics' (a subset of laion2B-en, filtered to images with an original size >= 512x512, estimated aesthetics score > 5.0, and an estimated watermark probability < 0.5.",
        ['sd-v1-2-full-ema.ckpt', 'sd-v1-2.ckpt'],
    ),
    "354e3712": ModelDataInfo(
        "SD1.3-fp32",
        "UNet EMA",
        "Stable Diffusion v1.3 EMA fp32",
        "data/unet/d4e70cc4-354e3712.bin",
        "d4e70cc4",
        "354e3712",
        "torch.float32",
        "859520964",
        "https://huggingface.co/CompVis/stable-diffusion-v-1-3-original",
        "https://huggingface.co/CompVis/stable-diffusion-v-1-3-original/resolve/main/sd-v1-3.ckpt",
        "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v-1-3 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 checkpoint and subsequently fine-tuned on 195,000 steps at resolution 512x512 on 'laion-improved-aesthetics' and 10\% dropping of the text-conditioning to improve classifier-free guidance sampling.",
        ['sd-v1-3-full-ema.ckpt', 'sd-v1-3.ckpt'],
    ),
    "7da99596": ModelDataInfo(
        "SD1.4-fp32",
        "UNet",
        "Stable Diffusion v1.4 EMA fp32",
        "data/unet/8b21d596-7da99596.bin",
        "8b21d596",
        "7da99596",
        "torch.float32",
        "859520964",
        "https://huggingface.co/CompVis/stable-diffusion-v-1-4-original",
        "https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt",
        "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v-1-4 checkpoint was initialized with the weights of the Stable-Diffusion-v-1-2 checkpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on 'laion-aesthetics v2 5+' and 10\% dropping of the text-conditioning to improve classifier-free guidance sampling.",
        ['stable-diffusion-v1-4', 'Taiyi-Stable-Diffusion-1B-Chinese-v0.1', 'sd-v1-4-full-ema.ckpt', 'sd-v1-4.ckpt'],
    ),
    "8d39156d": ModelDataInfo(
        "SD1.5-fp32",
        "UNet",
        "Stable Diffusion v1.5 EMA fp32",
        "data/unet/1fa05e1e-8d39156d.bin",
        "1fa05e1e",
        "8d39156d",
        "torch.float32",
        "859520964",
        "https://huggingface.co/runwayml/stable-diffusion-v1-5",
        "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt",
        "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-5 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 checkpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on 'laion-aesthetics v2 5+' and 10\% dropping of the text-conditioning to improve classifier-free guidance sampling.",
        ['v1-5-pruned-emaonly.ckpt', 'v1-5-pruned.ckpt'],
    ),
    "ab4e402c": ModelDataInfo(
        "JSD1.4-fp32",
        "UNet",
        "Japanese Stable Diffusion 1.4",
        "data/unet/cad4183c-ab4e402c.bin",
        "cad4183c",
        "ab4e402c",
        "torch.float32",
        "859520964",
        "https://huggingface.co/rinna/japanese-stable-diffusion",
        "https://huggingface.co/rinna/japanese-stable-diffusion/tree/main",
        "Japanese Stable Diffusion is a Japanese-specific latent text-to-image diffusion model capable of generating photo-realistic images given any text input. Trained on approximately 100 million images with Japanese captions, including the Japanese subset of LAION-5B.",
        ['JSD-v1-4.ckpt'],
    ),
    "fbd17571": ModelDataInfo(
        "WD1.3-fp16",
        "UNet",
        "Waifu Diffusion v1.3 EMA fp16",
        "data/unet/fbd17571-fbd17571.bin",
        "fbd17571",
        "fbd17571",
        "torch.float16",
        "859520964",
        "https://gist.github.com/harubaru/f727cedacae336d1f7877c4bbe2196e1",
        "https://huggingface.co/hakurei/waifu-diffusion-v1-3/blob/main/wd-v1-3-float16.ckpt",
        "Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning. The model originally used for fine-tuning is Stable Diffusion 1.4, which is a latent image diffusion model trained on LAION2B-en. The current model has been fine-tuned with a learning rate of 5.0e-6 for 10 epochs on 680k anime-styled images.",
        ['wd-v1-3-float16.ckpt'],
    ),
    "6a9f373b": ModelDataInfo(
        "WD1.3-fp32",
        "UNet",
        "Waifu Diffusion v1.3 EMA fp32",
        "data/unet/fbd17571-6a9f373b.bin",
        "fbd17571",
        "6a9f373b",
        "torch.float32",
        "859520964",
        "https://gist.github.com/harubaru/f727cedacae336d1f7877c4bbe2196e1",
        "https://huggingface.co/hakurei/waifu-diffusion-v1-3/blob/main/wd-v1-3-float32.ckpt",
        "Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning. The model originally used for fine-tuning is Stable Diffusion 1.4, which is a latent image diffusion model trained on LAION2B-en. The current model has been fine-tuned with a learning rate of 5.0e-6 for 10 epochs on 680k anime-styled images.",
        ['wd-v1-3-float32.ckpt', 'wd-v1-3-full-opt.ckpt', 'wd-v1-3-full.ckpt'],
    ),
    "5528a400": ModelDataInfo(
        "HD16-fp16",
        "UNet EMA",
        "Hentai Diffusion v16",
        "data/unet/5528a400-5528a400.bin",
        "5528a400",
        "5528a400",
        "torch.float16",
        "859520964",
        "https://github.com/Delcos/Hentai-Diffusion",
        "https://huggingface.co/Deltaadams/Hentai-Diffusion/resolve/main/HD-16.ckpt",
        "Hentai Diffusion has been made to focus not only on hentai, but better hands and better obscure poses.",
        ['HD-16.ckpt', 'Zack3D_Kinky-v1.ckpt'],
    ),
    "cedfeefb": ModelDataInfo(
        "NAI-fp32",
        "UNet EMA",
        "Novel AI Full leaked",
        "data/unet/ef56f0fa-cedfeefb.bin",
        "ef56f0fa",
        "cedfeefb",
        "torch.float32",
        "859520964",
        "https://blog.novelai.net/image-generation-announcement-807b3cf0afec",
        "magnet:?xt=urn:btih:5bde442da86265b670a3e5ea3163afad2c6f8ecc&dn=novelaileak",
        "NovelAI Image Generation is a proprietary model that was leaked.",
        ['nai-full-latest.ckpt', 'nai-full-pruned.ckpt', 'Anything-V3.0.ckpt'],
    ),
    "72a7f5e3": ModelDataInfo(
        "WikiArt2-fp16",
        "UNet",
        "sd-wikiart-v2",
        "data/unet/72a7f5e3-72a7f5e3.bin",
        "72a7f5e3",
        "72a7f5e3",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['sd-wikiart-v2'],
    ),
    "a6c1993d": ModelDataInfo(
        "BF_FB-fp16",
        "UNet",
        "bf_fb_v3_t4_b16_noadd-ema-pruned-fp16.ckpt",
        "data/unet/a6c1993d-a6c1993d.bin",
        "a6c1993d",
        "a6c1993d",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['bf_fb_v3_t4_b16_noadd-ema-pruned-fp16.ckpt'],
    ),
    "8592715d": ModelDataInfo(
        "BF_FB-fp32",
        "UNet",
        "bf_fb_v3_t4_b16_noadd-ema-pruned-fp32.ckpt",
        "data/unet/a6c1993d-8592715d.bin",
        "a6c1993d",
        "8592715d",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['bf_fb_v3_t4_b16_noadd-ema-pruned-fp32.ckpt'],
    ),
    "1fa05e1e": ModelDataInfo(
        "",
        "UNet EMA",
        "bondage3_24000.ckpt",
        "data/unet/1fa05e1e-1fa05e1e.bin",
        "1fa05e1e",
        "1fa05e1e",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['bondage3_24000.ckpt'],
    ),
    "1223ec31": ModelDataInfo(
        "",
        "UNet",
        "bukkake_20_training_images_2020_max_training_steps_woman_class_word.ckpt",
        "data/unet/1223ec31-1223ec31.bin",
        "1223ec31",
        "1223ec31",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['bukkake_20_training_images_2020_max_training_steps_woman_class_word.ckpt'],
    ),
    "c766bbed": ModelDataInfo(
        "",
        "UNet",
        "cookie_sd_pony_run_a12_datasetv5_300k_imgs_fp32.ckpt",
        "data/unet/c766bbed-c766bbed.bin",
        "c766bbed",
        "c766bbed",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['cookie_sd_pony_run_a12_datasetv5_300k_imgs_fp32.ckpt'],
    ),
    "796e54bf": ModelDataInfo(
        "Cyberpunk-Anime",
        "UNet",
        "Cyberpunk-Anime-Diffusion.ckpt",
        "data/unet/796e54bf-796e54bf.bin",
        "796e54bf",
        "796e54bf",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['Cyberpunk-Anime-Diffusion.ckpt'],
    ),
    "116a6e0a": ModelDataInfo(
        "",
        "CLIP Encoder",
        "DCAUV1.ckpt",
        "data/clip-text-encoder/116a6e0a-116a6e0a.bin",
        "116a6e0a",
        "116a6e0a",
        "torch.float16",
        "123060557",
        "",
        "",
        "",
        ['DCAUV1.ckpt'],
    ),
    "7acebf57": ModelDataInfo(
        "",
        "UNet",
        "DCAUV1.ckpt",
        "data/unet/7acebf57-7acebf57.bin",
        "7acebf57",
        "7acebf57",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['DCAUV1.ckpt'],
    ),
    "6f403e4a": ModelDataInfo(
        "",
        "UNet",
        "easter_e5.ckpt",
        "data/unet/6f403e4a-6f403e4a.bin",
        "6f403e4a",
        "6f403e4a",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['easter_e5.ckpt'],
    ),
    "55a0370c": ModelDataInfo(
        "",
        "UNet",
        "ema-only-epoch=000142.ckpt",
        "data/unet/0cd99ff3-55a0370c.bin",
        "0cd99ff3",
        "55a0370c",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['ema-only-epoch=000142.ckpt'],
    ),
    "89ab3fc3": ModelDataInfo(
        "",
        "CLIP Encoder",
        "f111.ckpt",
        "data/clip-text-encoder/5770c902-89ab3fc3.bin",
        "5770c902",
        "89ab3fc3",
        "torch.float32",
        "123060557",
        "",
        "",
        "",
        ['f111.ckpt'],
    ),
    "b2c3db47": ModelDataInfo(
        "",
        "VAE Encoder",
        "f111.ckpt",
        "data/vae-encoder/8159f4e7-b2c3db47.bin",
        "8159f4e7",
        "b2c3db47",
        "torch.float32",
        "34163664",
        "",
        "",
        "",
        ['f111.ckpt'],
    ),
    "bdd84115": ModelDataInfo(
        "",
        "VAE Decoder",
        "f111.ckpt",
        "data/vae-decoder/97dd98b1-bdd84115.bin",
        "97dd98b1",
        "bdd84115",
        "torch.float32",
        "49490199",
        "",
        "",
        "",
        ['f111.ckpt'],
    ),
    "9b3ee072": ModelDataInfo(
        "f111-fp32",
        "UNet",
        "f111.ckpt",
        "data/unet/23322baa-9b3ee072.bin",
        "23322baa",
        "9b3ee072",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['f111.ckpt'],
    ),
    "d996997f": ModelDataInfo(
        "",
        "CLIP Encoder",
        "f222.ckpt",
        "data/clip-text-encoder/5770c902-d996997f.bin",
        "5770c902",
        "d996997f",
        "torch.float32",
        "123060557",
        "",
        "",
        "",
        ['f222.ckpt'],
    ),
    "e642e16e": ModelDataInfo(
        "",
        "VAE Encoder",
        "f222.ckpt",
        "data/vae-encoder/8159f4e7-e642e16e.bin",
        "8159f4e7",
        "e642e16e",
        "torch.float32",
        "34163664",
        "",
        "",
        "",
        ['f222.ckpt'],
    ),
    "cbc41d9b": ModelDataInfo(
        "",
        "VAE Decoder",
        "f222.ckpt",
        "data/vae-decoder/97dd98b1-cbc41d9b.bin",
        "97dd98b1",
        "cbc41d9b",
        "torch.float32",
        "49490199",
        "",
        "",
        "",
        ['f222.ckpt'],
    ),
    "5a623fe1": ModelDataInfo(
        "f222-fp32",
        "UNet",
        "f222.ckpt",
        "data/unet/134a683d-5a623fe1.bin",
        "134a683d",
        "5a623fe1",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['f222.ckpt'],
    ),
    "06d65eaf": ModelDataInfo(
        "",
        "UNet EMA",
        "furry_epoch4.ckpt",
        "data/unet/06d65eaf-06d65eaf.bin",
        "06d65eaf",
        "06d65eaf",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['furry_epoch4.ckpt'],
    ),
    "9355b422": ModelDataInfo(
        "",
        "UNet EMA",
        "gape22_yiffy15.ckpt",
        "data/unet/9355b422-9355b422.bin",
        "9355b422",
        "9355b422",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['gape22_yiffy15.ckpt'],
    ),
    "3dde4f97": ModelDataInfo(
        "",
        "UNet",
        "gape60.ckpt",
        "data/unet/9d0e893c-3dde4f97.bin",
        "9d0e893c",
        "3dde4f97",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['gape60.ckpt'],
    ),
    "61a460c5": ModelDataInfo(
        "GG-fp32",
        "UNet",
        "gg1342_testrun1_pruned.ckpt",
        "data/unet/3b67e187-61a460c5.bin",
        "3b67e187",
        "61a460c5",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['gg1342_testrun1_pruned.ckpt'],
    ),
    "191d2210": ModelDataInfo(
        "",
        "VAE Encoder",
        "HD-16.ckpt",
        "data/vae-encoder/10acfaa6-191d2210.bin",
        "10acfaa6",
        "191d2210",
        "torch.float32",
        "34163664",
        "",
        "",
        "",
        ['HD-16.ckpt'],
    ),
    "05e8c698": ModelDataInfo(
        "",
        "VAE Decoder",
        "HD-16.ckpt",
        "data/vae-decoder/36abe8b0-05e8c698.bin",
        "36abe8b0",
        "05e8c698",
        "torch.float32",
        "49490199",
        "",
        "",
        "",
        ['HD-16.ckpt'],
    ),
    "009519cc": ModelDataInfo(
        "",
        "CLIP Encoder",
        "Hiten girl_anime_8k_wallpaper_4k.ckpt",
        "data/clip-text-encoder/009519cc-009519cc.bin",
        "009519cc",
        "009519cc",
        "torch.float16",
        "123060557",
        "",
        "",
        "",
        ['Hiten girl_anime_8k_wallpaper_4k.ckpt'],
    ),
    "325017eb": ModelDataInfo(
        "",
        "UNet",
        "Hiten girl_anime_8k_wallpaper_4k.ckpt",
        "data/unet/325017eb-325017eb.bin",
        "325017eb",
        "325017eb",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['Hiten girl_anime_8k_wallpaper_4k.ckpt'],
    ),
    "e5611d52": ModelDataInfo(
        "",
        "CLIP Encoder",
        "JSD-v1-4.ckpt",
        "data/clip-text-encoder/69cfcfc3-e5611d52.bin",
        "69cfcfc3",
        "e5611d52",
        "torch.float32, torch.int64",
        "109691213",
        "",
        "",
        "",
        ['JSD-v1-4.ckpt'],
    ),
    "4e03132f": ModelDataInfo(
        "",
        "UNet EMA",
        "last-pruned.ckpt",
        "data/unet/4e03132f-4e03132f.bin",
        "4e03132f",
        "4e03132f",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['last-pruned.ckpt'],
    ),
    "2ee3899d": ModelDataInfo(
        "",
        "UNet EMA",
        "LD-70k-1e-pruned.ckpt",
        "data/unet/2ee3899d-2ee3899d.bin",
        "2ee3899d",
        "2ee3899d",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['LD-70k-1e-pruned.ckpt'],
    ),
    "ab8f2115": ModelDataInfo(
        "",
        "UNet EMA",
        "Lewd-diffusion-pruned.ckpt",
        "data/unet/ab8f2115-ab8f2115.bin",
        "ab8f2115",
        "ab8f2115",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['Lewd-diffusion-pruned.ckpt'],
    ),
    "d81fcda3": ModelDataInfo(
        "",
        "UNet",
        "LOKEAN_MISSIONARY_POV.ckpt",
        "data/unet/ebfad7a8-d81fcda3.bin",
        "ebfad7a8",
        "d81fcda3",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['LOKEAN_MISSIONARY_POV.ckpt'],
    ),
    "15e554a5": ModelDataInfo(
        "",
        "UNet",
        "LOKEAN_PUPPYSTYLE_POV.ckpt",
        "data/unet/721f9e07-15e554a5.bin",
        "721f9e07",
        "15e554a5",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['LOKEAN_PUPPYSTYLE_POV.ckpt'],
    ),
    "130d74ce": ModelDataInfo(
        "",
        "UNet",
        "Mixed.ckpt",
        "data/unet/ab4ac737-130d74ce.bin",
        "ab4ac737",
        "130d74ce",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['Mixed.ckpt'],
    ),
    "b44c8b0e": ModelDataInfo(
        "",
        "CLIP Encoder",
        "pachu_artwork_style_v1_iter8000.ckpt",
        "data/clip-text-encoder/13835133-b44c8b0e.bin",
        "13835133",
        "b44c8b0e",
        "torch.float32, torch.int64",
        "123060557",
        "",
        "",
        "",
        ['pachu_artwork_style_v1_iter8000.ckpt'],
    ),
    "62ab9f03": ModelDataInfo(
        "VAEDec1_5mse_fp32",
        "VAE Decoder",
        "VAE Decoder 1.5 MSE 840000 steps",
        "data/vae-decoder/75335bbc-62ab9f03.bin",
        "75335bbc",
        "62ab9f03",
        "torch.float32",
        "49490199",
        "https://huggingface.co/stabilityai/sd-vae-ft-mse-original",
        "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt",
        "This VAE was resumed from the 1.5 EMA decoder and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs.",
        ['pachu_artwork_style_v1_iter8000.ckpt', 'SF_EB_1.0_ema_vae.ckpt', 'vae-ft-mse-840000-ema-pruned.ckpt'],
    ),
    "b103a9fc": ModelDataInfo(
        "",
        "UNet",
        "pachu_artwork_style_v1_iter8000.ckpt",
        "data/unet/6706c7a0-b103a9fc.bin",
        "6706c7a0",
        "b103a9fc",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['pachu_artwork_style_v1_iter8000.ckpt'],
    ),
    "b8a1e44c": ModelDataInfo(
        "",
        "UNet",
        "pyros-bj-v1-0.ckpt",
        "data/unet/b8a1e44c-b8a1e44c.bin",
        "b8a1e44c",
        "b8a1e44c",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['pyros-bj-v1-0.ckpt'],
    ),
    "6910d82d": ModelDataInfo(
        "R34e4-fp16",
        "UNet",
        "r34_e4.ckpt",
        "data/unet/6910d82d-6910d82d.bin",
        "6910d82d",
        "6910d82d",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['r34_e4.ckpt'],
    ),
    "72e47874": ModelDataInfo(
        "",
        "CLIP Encoder",
        "robo-diffusion-v1.ckpt",
        "data/clip-text-encoder/ed8e40b3-72e47874.bin",
        "ed8e40b3",
        "72e47874",
        "torch.float32, torch.int64",
        "123060557",
        "",
        "",
        "",
        ['robo-diffusion-v1.ckpt'],
    ),
    "cf749104": ModelDataInfo(
        "",
        "UNet",
        "robo-diffusion-v1.ckpt",
        "data/unet/3a6bfe5c-cf749104.bin",
        "3a6bfe5c",
        "cf749104",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['robo-diffusion-v1.ckpt'],
    ),
    "eaa3719c": ModelDataInfo(
        "SD1.5inpaint-fp32",
        "UNet",
        "sd-v1-5-inpainting.ckpt",
        "data/unet/6e119516-eaa3719c.bin",
        "6e119516",
        "eaa3719c",
        "torch.float32",
        "859535364",
        "",
        "",
        "",
        ['sd-v1-5-inpainting.ckpt'],
    ),
    "0c71c262": ModelDataInfo(
        "Trinart2_115k",
        "UNet",
        "trinart2_step115000.ckpt",
        "data/unet/0c71c262-0c71c262.bin",
        "0c71c262",
        "0c71c262",
        "torch.float16",
        "859520964",
        "https://huggingface.co/naclbit/trinart_stable_diffusion_v2",
        "https://huggingface.co/naclbit/trinart_stable_diffusion_v2/resolve/main/trinart2_step115000.ckpt",
        "This model is NOT the 19.2M images Characters Model on TrinArt, but an improved version of the original trinsama Twitter bot model. This model is intended to retain the original SD's aesthetics as much as possible while nudging the model to anime/manga style.",
        ['trinart2_step115000.ckpt'],
    ),
    "782de700": ModelDataInfo(
        "Trinart2_60k",
        "UNet",
        "trinart2_step60000.ckpt",
        "data/unet/782de700-782de700.bin",
        "782de700",
        "782de700",
        "torch.float16",
        "859520964",
        "https://huggingface.co/naclbit/trinart_stable_diffusion_v2",
        "https://huggingface.co/naclbit/trinart_stable_diffusion_v2/resolve/main/trinart2_step60000.ckpt",
        "This model is NOT the 19.2M images Characters Model on TrinArt, but an improved version of the original trinsama Twitter bot model. This model is intended to retain the original SD's aesthetics as much as possible while nudging the model to anime/manga style.",
        ['trinart2_step60000.ckpt'],
    ),
    "976c48c9": ModelDataInfo(
        "Trinart2_95k",
        "UNet",
        "trinart2_step95000.ckpt",
        "data/unet/976c48c9-976c48c9.bin",
        "976c48c9",
        "976c48c9",
        "torch.float16",
        "859520964",
        "https://huggingface.co/naclbit/trinart_stable_diffusion_v2",
        "https://huggingface.co/naclbit/trinart_stable_diffusion_v2/resolve/main/trinart2_step95000.ckpt",
        "This model is NOT the 19.2M images Characters Model on TrinArt, but an improved version of the original trinsama Twitter bot model. This model is intended to retain the original SD's aesthetics as much as possible while nudging the model to anime/manga style.",
        ['trinart2_step95000.ckpt'],
    ),
    "fb4f2006": ModelDataInfo(
        "Trinart_19M",
        "UNet",
        "trinart_characters_it4_v1.ckpt",
        "data/unet/fb4f2006-fb4f2006.bin",
        "fb4f2006",
        "fb4f2006",
        "torch.float16",
        "859520964",
        "https://huggingface.co/naclbit/trinart_characters_19.2m_stable_diffusion_v1",
        "https://huggingface.co/naclbit/trinart_characters_19.2m_stable_diffusion_v1/resolve/main/trinart_characters_it4_v1.ckpt",
        "This is a stable diffusion v1-based model trained by roughly 19.2M anime/manga style images (pre-rolled augmented images included) plus final finetuning by about 50,000 images. This model seeks for a sweet spot between artistic style versatility and anatomical quality within the given model spec of SDv1.",
        ['trinart_characters_it4_v1.ckpt'],
    ),
    "09da1114": ModelDataInfo(
        "",
        "UNet EMA",
        "yiffy-e18.ckpt",
        "data/unet/09da1114-09da1114.bin",
        "09da1114",
        "09da1114",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['yiffy-e18.ckpt'],
    ),
    "ef4ab8be": ModelDataInfo(
        "",
        "CLIP Encoder",
        "bukkake_20_training_images_2020_max_training_steps_woman_class_word.ckpt",
        "data/clip-text-encoder/ef4ab8be-ef4ab8be.bin",
        "ef4ab8be",
        "ef4ab8be",
        "torch.float16",
        "123060557",
        "",
        "",
        "",
        ['bukkake_20_training_images_2020_max_training_steps_woman_class_word.ckpt'],
    ),
    "0ec000aa": ModelDataInfo(
        "",
        "CLIP Encoder",
        "cookie_sd_pony_run_a12_datasetv5_300k_imgs_fp32.ckpt",
        "data/clip-text-encoder/0ec000aa-0ec000aa.bin",
        "0ec000aa",
        "0ec000aa",
        "torch.float32, torch.int64",
        "123060557",
        "",
        "",
        "",
        ['cookie_sd_pony_run_a12_datasetv5_300k_imgs_fp32.ckpt'],
    ),
    "67084cde": ModelDataInfo(
        "",
        "VAE Encoder",
        "cookie_sd_pony_run_a12_datasetv5_300k_imgs_fp32.ckpt",
        "data/vae-encoder/67084cde-67084cde.bin",
        "67084cde",
        "67084cde",
        "torch.float32",
        "34163664",
        "",
        "",
        "",
        ['cookie_sd_pony_run_a12_datasetv5_300k_imgs_fp32.ckpt'],
    ),
    "fab450bf": ModelDataInfo(
        "",
        "VAE Decoder",
        "cookie_sd_pony_run_a12_datasetv5_300k_imgs_fp32.ckpt",
        "data/vae-decoder/fab450bf-fab450bf.bin",
        "fab450bf",
        "fab450bf",
        "torch.float32",
        "49490199",
        "",
        "",
        "",
        ['cookie_sd_pony_run_a12_datasetv5_300k_imgs_fp32.ckpt'],
    ),
    "e05242e9": ModelDataInfo(
        "",
        "CLIP Encoder",
        "Cyberpunk-Anime-Diffusion.ckpt",
        "data/clip-text-encoder/e05242e9-e05242e9.bin",
        "e05242e9",
        "e05242e9",
        "torch.float16",
        "123060557",
        "",
        "",
        "",
        ['Cyberpunk-Anime-Diffusion.ckpt'],
    ),
    "2192e418": ModelDataInfo(
        "AnythingV3",
        "CLIP Encoder",
        "Anything-V3.0.ckpt",
        "data/clip-text-encoder/b06fd5fb-2192e418.bin",
        "b06fd5fb",
        "2192e418",
        "torch.float32",
        "123060557",
        "",
        "",
        "",
        ['Anything-V3.0.ckpt'],
    ),
    "d25826eb": ModelDataInfo(
        "AnythingV3",
        "VAE Encoder",
        "Anything-V3.0.ckpt",
        "data/vae-encoder/191c1b9c-d25826eb.bin",
        "191c1b9c",
        "d25826eb",
        "torch.float32",
        "34163664",
        "",
        "",
        "",
        ['Anything-V3.0.ckpt'],
    ),
    "c73aef7b": ModelDataInfo(
        "AnythingV3",
        "VAE Decoder",
        "Anything-V3.0.ckpt",
        "data/vae-decoder/926ffdd2-c73aef7b.bin",
        "926ffdd2",
        "c73aef7b",
        "torch.float32",
        "49490199",
        "",
        "",
        "",
        ['Anything-V3.0.ckpt'],
    ),
    "f7a5e055": ModelDataInfo(
        "Cafe_InstagramE9",
        "UNet",
        "cafe-instagram-unofficial-test-epoch-9-140k-images-fp32.ckpt",
        "data/unet/8276ed69-f7a5e055.bin",
        "8276ed69",
        "f7a5e055",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['cafe-instagram-unofficial-test-epoch-9-140k-images-fp32.ckpt'],
    ),
    "63ff7ec0": ModelDataInfo(
        "",
        "UNet",
        "SF_EB_1.0_ema_vae.ckpt",
        "data/unet/554c51e4-63ff7ec0.bin",
        "554c51e4",
        "63ff7ec0",
        "torch.float32",
        "859520964",
        "",
        "",
        "",
        ['SF_EB_1.0_ema_vae.ckpt'],
    ),
    "6e38eefd": ModelDataInfo(
        "VAEDec1_5ema_fp32",
        "VAE Decoder",
        "VAE Decoder 1.5 EMA 560000 steps",
        "data/vae-decoder/740753ae-6e38eefd.bin",
        "740753ae",
        "6e38eefd",
        "torch.float32",
        "49490199",
        "https://huggingface.co/stabilityai/sd-vae-ft-mse-original",
        "https://huggingface.co/stabilityai/sd-vae-ft-ema-original/resolve/main/vae-ft-ema-560000-ema-pruned.ckpt",
        "This decoder was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).",
        ['vae-ft-ema-560000-ema-pruned.ckpt'],
    ),
    "64ec6f3c": ModelDataInfo(
        "",
        "CLIP Encoder",
        "arcane-diffusion-5k.ckpt",
        "data/clip-text-encoder/64ec6f3c-64ec6f3c.bin",
        "64ec6f3c",
        "64ec6f3c",
        "torch.float16",
        "123060557",
        "",
        "",
        "",
        ['arcane-diffusion-5k.ckpt'],
    ),
    "5408e56e": ModelDataInfo(
        "Arcane5k",
        "UNet",
        "arcane-diffusion-5k.ckpt",
        "data/unet/5408e56e-5408e56e.bin",
        "5408e56e",
        "5408e56e",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['arcane-diffusion-5k.ckpt'],
    ),
    "f6bec02d": ModelDataInfo(
        "ArcaneV2",
        "UNet",
        "arcane-diffusion-v2.ckpt",
        "data/unet/f6bec02d-f6bec02d.bin",
        "f6bec02d",
        "f6bec02d",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['arcane-diffusion-v2.ckpt'],
    ),
    "8a31166c": ModelDataInfo(
        "ArcaneV3",
        "CLIP Encoder",
        "arcane-diffusion-v3.ckpt",
        "data/clip-text-encoder/8a31166c-8a31166c.bin",
        "8a31166c",
        "8a31166c",
        "torch.float16",
        "123060557",
        "",
        "",
        "",
        ['arcane-diffusion-v3.ckpt'],
    ),
    "5b364723": ModelDataInfo(
        "ArcaneV3",
        "UNet",
        "arcane-diffusion-v3.ckpt",
        "data/unet/5b364723-5b364723.bin",
        "5b364723",
        "5b364723",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['arcane-diffusion-v3.ckpt'],
    ),
    "590d410d": ModelDataInfo(
        "ClassicAnimV1",
        "CLIP Encoder",
        "classicAnim-v1.ckpt",
        "data/clip-text-encoder/590d410d-590d410d.bin",
        "590d410d",
        "590d410d",
        "torch.float16",
        "123060557",
        "",
        "",
        "",
        ['classicAnim-v1.ckpt'],
    ),
    "75335bbc": ModelDataInfo(
        "VAEDec1_5mse_fp16",
        "VAE Decoder",
        "VAE Decoder 1.5 MSE 840000 steps",
        "data/vae-decoder/75335bbc-75335bbc.bin",
        "75335bbc",
        "75335bbc",
        "torch.float16",
        "49490199",
        "https://huggingface.co/stabilityai/sd-vae-ft-mse-original",
        "https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt",
        "This VAE was resumed from the 1.5 EMA decoder and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs.",
        ['classicAnim-v1.ckpt', 'moDi-v1-pruned.ckpt', 'eldenring-v2-pruned.ckpt', 'eldenRing-v3-pruned.ckpt'],
    ),
    "c84d108f": ModelDataInfo(
        "ClassicAnimV1",
        "UNet",
        "classicAnim-v1.ckpt",
        "data/unet/c84d108f-c84d108f.bin",
        "c84d108f",
        "c84d108f",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['classicAnim-v1.ckpt'],
    ),
    "6162e948": ModelDataInfo(
        "EldenRingV1",
        "UNet",
        "eldenring-v1-pruned.ckpt",
        "data/unet/6162e948-6162e948.bin",
        "6162e948",
        "6162e948",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['eldenring-v1-pruned.ckpt'],
    ),
    "e6e51ee0": ModelDataInfo(
        "ModernDisneyV1",
        "CLIP Encoder",
        "moDi-v1-pruned.ckpt",
        "data/clip-text-encoder/e6e51ee0-e6e51ee0.bin",
        "e6e51ee0",
        "e6e51ee0",
        "torch.float16",
        "123060557",
        "",
        "",
        "",
        ['moDi-v1-pruned.ckpt'],
    ),
    "2b27501d": ModelDataInfo(
        "ModernDisneyV1",
        "UNet",
        "moDi-v1-pruned.ckpt",
        "data/unet/2b27501d-2b27501d.bin",
        "2b27501d",
        "2b27501d",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['moDi-v1-pruned.ckpt'],
    ),
    "1e50dce5": ModelDataInfo(
        "EldenRingV2",
        "CLIP Encoder",
        "eldenring-v2-pruned.ckpt",
        "data/clip-text-encoder/1e50dce5-1e50dce5.bin",
        "1e50dce5",
        "1e50dce5",
        "torch.float16",
        "123060557",
        "",
        "",
        "",
        ['eldenring-v2-pruned.ckpt'],
    ),
    "1296dd90": ModelDataInfo(
        "EldenRingV2",
        "UNet",
        "eldenring-v2-pruned.ckpt",
        "data/unet/1296dd90-1296dd90.bin",
        "1296dd90",
        "1296dd90",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['eldenring-v2-pruned.ckpt'],
    ),
    "ed0fa8d2": ModelDataInfo(
        "EldenRingV3",
        "CLIP Encoder",
        "eldenRing-v3-pruned.ckpt",
        "data/clip-text-encoder/ed0fa8d2-ed0fa8d2.bin",
        "ed0fa8d2",
        "ed0fa8d2",
        "torch.float16",
        "123060557",
        "",
        "",
        "",
        ['eldenRing-v3-pruned.ckpt'],
    ),
    "6c2a99f3": ModelDataInfo(
        "EldenRingV3",
        "UNet",
        "eldenRing-v3-pruned.ckpt",
        "data/unet/6c2a99f3-6c2a99f3.bin",
        "6c2a99f3",
        "6c2a99f3",
        "torch.float16",
        "859520964",
        "",
        "",
        "",
        ['eldenRing-v3-pruned.ckpt'],
    )
})